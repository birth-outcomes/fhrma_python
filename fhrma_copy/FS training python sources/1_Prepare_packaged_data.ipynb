{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 Prepare packaged data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/lSG9KEgMfS+M0lE+nhul",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsb-fmm/FHRMA/blob/master/FS%20training%20python%20sources/1_Prepare_packaged_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset and prepare it in compact matrix by concatenating small recordings in constant width units:\n",
        "\n",
        "Prepare datasets for FSDop, FSMHR, and FSScalp.\n",
        "\n",
        "Associated paper in https://www.preprints.org/manuscript/202207.0131/v1\n",
        "\n",
        "---\n",
        "FHR Morphological Analysis Toolbox Copyright (C) 2022 Samuel Boudet, Faculté de Médecine et Maïeutique,\n",
        "samuel.boudet@gmail.com\n",
        "\n",
        "This file is part of FHR Morphological Analysis Toolbox\n",
        "\n",
        "FHR Morphological Analysis Toolbox is free software: you can redistribute it and/or modify\n",
        "it under the terms of the GNU General Public License as published by\n",
        "the Free Software Foundation, either version 3 of the License, or\n",
        "(at your option) any later version.\n",
        "\n",
        " FHR Morphological Analysis Toolbox is distributed in the hope that it will be useful,\n",
        "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "GNU General Public License for more details.\n",
        "\n",
        "You should have received a copy of the GNU General Public License\n",
        "along with this program.  If not, see <http://www.gnu.org/licenses/>."
      ],
      "metadata": {
        "id": "RNd5iESA2__j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvH3EAJfo-Sn"
      },
      "source": [
        "import tensorflow as tf\n",
        "import array\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import pickle\n",
        "from IPython.core.debugger import set_trace\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUm0EnidLa82",
        "outputId": "910c2a4a-dfc0-4b3f-b8e8-757f4eb4b095"
      },
      "source": [
        "#The two firsts lines are for Colab to write the packaged data in your google drive.\n",
        "#Fill free to change for other platforms (but change also in other files)\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "basefolder=\"drive/My Drive/FHRMA-Training-FS/\"\n",
        "\n",
        "!wget https://github.com/utsb-fmm/FHRMA/raw/master/FS%20training%20python%20sources/dataV8.zip\n",
        "!unzip dataV8.zip\n",
        "!cp DiffMF.dat 'drive/My Drive/FHRMA-Training-FS/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n",
            "--2022-07-20 18:57:21--  https://github.com/utsb-fmm/FHRMA/raw/master/FS%20training%20python%20sources/dataV8.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2022-07-20 18:57:21 ERROR 404: Not Found.\n",
            "\n",
            "unzip:  cannot find or open dataV8.zip, dataV8.zip.zip or dataV8.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I For FSMHR\n"
      ],
      "metadata": {
        "id": "rCAgqJifHbBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nd=733\n",
        "maxrow=200\n",
        "#Package the MHR signals in units of length \"samp\", \n",
        "# the number of units will be minimized for training dataset and will be\n",
        "#maxrowval for validation dataset\n",
        "#maxrowval and number of unit in train dataset should be a multiple of your batch size\n",
        "#so samp should be set to have the training dataset unit number is a multiple of batch size, and samp should be greater than the longest signal (to include it)\n",
        "#We recommend setting the highest possible batch_size to lower computation time, but a too high batch_size will saturate GPU (or TPU) memory. \n",
        "#For TPU V3 and V2, batch_size should be a multiple of 8 (number of core TPU)\n",
        "#Denpending of training computer and memory you should adjust those parameters\n",
        "\n",
        "# For GPU\n",
        "samp=57700 \n",
        "maxrowval=13\n",
        "\n",
        "u8=array.array(\"B\")\n",
        "Lfile=os.path.getsize(\"dataV8/isval.dat\")\n",
        "with open(\"dataV8/isval.dat\", \"rb\") as binary_file:\n",
        "    u8.fromfile(binary_file,Lfile) \n",
        "isval0=np.asarray(u8.tolist())>0\n",
        "\n",
        "X0=();Y0=();L=();isval=();\n",
        "for i in range(nd):\n",
        "    Lt=os.path.getsize(\"dataV8/dop\"+(\"%03d\" % (i+1))+\".dop\")//10\n",
        "    u16=array.array(\"H\")\n",
        "    with open(\"dataV8/dop\"+(\"%03d\" % (i+1))+\".dop\", \"rb\") as binary_file:\n",
        "        u16.fromfile(binary_file,Lt*5)\n",
        "    data=np.asarray(u16.tolist()).reshape(-1,5)/4\n",
        "\n",
        "    isFalse=(data[:,2]<1)\n",
        "    docare=(data[:,2]!=1)     \n",
        "    isFalseM=(data[:,3]<1)\n",
        "    docareM=(data[:,3]!=1)       \n",
        "\n",
        "    if sum(docareM)>0: #+sum(docare) if FSDop and FSMHR train simultaneously\n",
        "        k=np.sqrt(len(docareM)/(sum(docareM)))\n",
        "        docareM=docareM*k       \n",
        "    \n",
        "        Ynp=np.concatenate((docare.reshape(1,-1,1),isFalse.reshape(1,-1,1),docareM.reshape(1,-1,1),isFalseM.reshape(1,-1,1)),axis=2)\n",
        "        NotLoss=(data>0).astype(float)\n",
        "        data=np.concatenate(((data[:,0:1]-120)/60*NotLoss[:,0:1],NotLoss[:,0:1],(data[:,1:2]-120)/60*NotLoss[:,1:2],NotLoss[:,1:2],data[:,4:5]),axis=1)\n",
        "        Y0+=(Ynp,)\n",
        "        X0+=(data,)   \n",
        "        isval+=(isval0[i],)\n",
        "        L+=(Lt,)\n",
        "\n",
        "X0trainval=[()  for k in range(2)];Y0trainval=[() for k in range(2)];L0trainval=[()  for k in range(2)];\n",
        "X0trainval[0]=[X0[k] for k in np.where(np.logical_not(isval))[0]]\n",
        "Y0trainval[0]=[Y0[k] for k in np.where(np.logical_not(isval))[0]]\n",
        "L0trainval[0]=np.asarray([L[k] for k in np.where(np.logical_not(isval))[0]])\n",
        "X0trainval[1]=[X0[k] for k in np.where(isval)[0]]\n",
        "Y0trainval[1]=[Y0[k] for k in np.where(isval)[0]]\n",
        "L0trainval[1]=np.asarray([L[k] for k in np.where(isval)[0]])\n",
        "\n",
        "nval=len(L0trainval[1])\n",
        "\n",
        "X=[np.zeros((maxrow,samp,6)) for k in range(2)]\n",
        "Y=[np.zeros((maxrow,samp,4)) for k in range(2)]\n",
        "\n",
        "ListRec=[ [(),]*maxrow for k in range(2)]\n",
        "LengthRec=[ [(),]*maxrow for k in range(2)]\n",
        "\n",
        "if max(L0trainval[0])>samp:\n",
        "    print(\"Warning: a training recording is not included\")\n",
        "    print(np.where(L0trainval[0]>samp))\n",
        "# For train dataset, we fill the lines successyvely by adding the longer possible recording at the end of line until no recording fill\n",
        "i=0\n",
        "while min(L0trainval[0])<=samp:\n",
        "    X[0][i,0,5]=1\n",
        "    j=1\n",
        "    while samp-j-1>=min(L0trainval[0]):\n",
        "        k=np.argmax(L0trainval[0]*(L0trainval[0]<=samp-j-1))\n",
        "        ListRec[0][i]+=(k,)\n",
        "        LengthRec[0][i]+=(L0trainval[0][k],)\n",
        "        X[0][i,j:j+L0trainval[0][k],0:5]=X0trainval[0][k]\n",
        "        X[0][i,j+L0trainval[0][k],5]=1\n",
        "        Y[0][i,j:j+L0trainval[0][k],0:4]=Y0trainval[0][k]\n",
        "        j=j+L0trainval[0][k]+1\n",
        "        L0trainval[0][k]=samp+1\n",
        "\n",
        "    X[0][i,j:,5]=1\n",
        "    i=i+1\n",
        "X[0]=X[0][:i,:,:]\n",
        "Y[0]=Y[0][:i,:,:]\n",
        "ListRec[0]=ListRec[0][:i]\n",
        "LengthRec[0]=LengthRec[0][:i]\n",
        "\n",
        "# For validation dataset, we first put the longer recordings \n",
        "X[1]=X[1][:maxrowval,:,:]\n",
        "Y[1]=Y[1][:maxrowval,:,:]\n",
        "ListRec[1]=ListRec[1][:maxrowval]\n",
        "LengthRec[1]=LengthRec[1][:maxrowval]    \n",
        "CurrFill=np.ones(maxrowval,dtype=np.int32)\n",
        "X[1][:,0,5]=1\n",
        "while min(L0trainval[1])<=samp: #allocated rec hav L0 = samp+1\n",
        "    k=np.argmax(L0trainval[1]*(L0trainval[1]<=samp)) #choose longer rec not allocated\n",
        "    pos=np.argmin(CurrFill) # Search the less filled line\n",
        "    #print(\"pos %d rec %d of length %d\" % (pos,k,L0trainval[1][k]))\n",
        "    ListRec[1][pos]+=(k,)\n",
        "    LengthRec[1][pos]+=(L0trainval[1][k],)\n",
        "    X[1][pos,CurrFill[pos]:CurrFill[pos]+L0trainval[1][k],0:5]=X0trainval[1][k]\n",
        "    X[1][pos,CurrFill[pos]+L0trainval[1][k],5]=1\n",
        "    Y[1][pos,CurrFill[pos]:CurrFill[pos]+L0trainval[1][k],0:4]=Y0trainval[1][k]\n",
        "    CurrFill[pos]+=(L0trainval[1][k]+1)\n",
        "    L0trainval[1][k]=samp+1\n",
        "\n",
        "if np.any(L0trainval[1]!=samp+1):\n",
        "    print(\"Warning: a test recording is not included\")\n",
        "    print(np.where(L0trainval[1]!=samp+1))\n",
        "\n",
        "for i in range(maxrowval):\n",
        "    X[1][i,CurrFill[i]:,5]=1\n",
        "\n",
        "(X_train,Y_train,X_val,Y_val)=(X[0],Y[0],X[1],Y[1])\n",
        "Y_val[:,:,2]=Y_val[:,:,2]/np.sum(Y_val[:,:,2])\n",
        "Y_train[:,:,2]=Y_train[:,:,2]/np.sum(Y_train[:,:,2]) \n",
        "\n",
        "with open(basefolder+'dataV8MHR%d-%d.pkl'%(X_train.shape[0],X_val.shape[0]), 'wb') as f:  \n",
        "    pickle.dump([X_train,Y_train,X_val,Y_val,ListRec,LengthRec], f)\n",
        "\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5tXh-0SE6_G",
        "outputId": "62a117e1-35f4-47cf-a320-1e4d7cab455b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 57700, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II For FSDop"
      ],
      "metadata": {
        "id": "Pb3FKr7pHy9g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG3E047sLtFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f91e76-0c73-4724-af6a-49cf9eae978c"
      },
      "source": [
        "nd=733\n",
        "maxrow=200\n",
        "#Package the Doppler signals in units of length \"samp\", \n",
        "# the number of units will be minimized for training dataset and will be\n",
        "#maxrowval for validation dataset\n",
        "#maxrowval and number of unit in train dataset should be a multiple of your batch size\n",
        "#so samp should be set to have the training dataset unit number is a multiple of batch size, and samp should be greater than the longest signal (to include it)\n",
        "#A too high batch_size will saturate GPU (or TPU) memory. We recommend setting the highest possible batch_size to lower computation time\n",
        "#For TPU V3 and V2, batch_size should be a multiple of 8 (number of core TPU)\n",
        "#Denpending of training computer and memory you should adjust those parameter\n",
        "\n",
        "# For TPU V3\n",
        "#samp=64800 # min for having 80 on train lines; batch_size 40\n",
        "#maxrowval=40 \n",
        "\n",
        "#For TPU V2 \n",
        "#samp=72000 # min for having 72 on train lines; batch_size 24\n",
        "#maxrowval=24\n",
        "\n",
        "# For GPU\n",
        "samp=64800 # min for having 80 on train lines; batch_size 20\n",
        "maxrowval=20 \n",
        "\n",
        "u8=array.array(\"B\")\n",
        "Lfile=os.path.getsize(\"dataV8/isval.dat\")\n",
        "with open(\"dataV8/isval.dat\", \"rb\") as binary_file:\n",
        "    u8.fromfile(binary_file,Lfile) \n",
        "isval0=np.asarray(u8.tolist())>0\n",
        "\n",
        "X0=();Y0=();L=();isval=();\n",
        "for i in range(nd):\n",
        "    Lt=os.path.getsize(\"dataV8/dop\"+(\"%03d\" % (i+1))+\".dop\")//10\n",
        "    u16=array.array(\"H\")\n",
        "    with open(\"dataV8/dop\"+(\"%03d\" % (i+1))+\".dop\", \"rb\") as binary_file:\n",
        "        u16.fromfile(binary_file,Lt*5)\n",
        "    data=np.asarray(u16.tolist()).reshape(-1,5)/4\n",
        "\n",
        "    isFalse=(data[:,2]<1)\n",
        "    docare=(data[:,2]!=1)     \n",
        "    isFalseM=(data[:,3]<1)\n",
        "    docareM=(data[:,3]!=1)       \n",
        "\n",
        "    if sum(docare)>0:\n",
        "        k=np.sqrt(len(docare)/(sum(docare)))\n",
        "        docare=docare*k       \n",
        "\n",
        "        Ynp=np.concatenate((docare.reshape(1,-1,1),isFalse.reshape(1,-1,1),docareM.reshape(1,-1,1),isFalseM.reshape(1,-1,1)),axis=2)\n",
        "        NotLoss=(data>0).astype(float)\n",
        "        data=np.concatenate(((data[:,0:1]-120)/60*NotLoss[:,0:1],NotLoss[:,0:1],(data[:,1:2]-120)/60*NotLoss[:,1:2],NotLoss[:,1:2],data[:,4:5]),axis=1)\n",
        "        Y0+=(Ynp,)\n",
        "        X0+=(data,)   \n",
        "        isval+=(isval0[i],)\n",
        "        L+=(Lt,)\n",
        "\n",
        "X0trainval=[()  for k in range(2)];Y0trainval=[() for k in range(2)];L0trainval=[()  for k in range(2)];\n",
        "X0trainval[0]=[X0[k] for k in np.where(np.logical_not(isval))[0]]\n",
        "Y0trainval[0]=[Y0[k] for k in np.where(np.logical_not(isval))[0]]\n",
        "L0trainval[0]=np.asarray([L[k] for k in np.where(np.logical_not(isval))[0]])\n",
        "X0trainval[1]=[X0[k] for k in np.where(isval)[0]]\n",
        "Y0trainval[1]=[Y0[k] for k in np.where(isval)[0]]\n",
        "L0trainval[1]=np.asarray([L[k] for k in np.where(isval)[0]])\n",
        "\n",
        "nval=len(L0trainval[1])\n",
        "\n",
        "X=[np.zeros((maxrow,samp,6)) for k in range(2)]\n",
        "Y=[np.zeros((maxrow,samp,4)) for k in range(2)]\n",
        "\n",
        "ListRec=[ [(),]*maxrow for k in range(2)]\n",
        "LengthRec=[ [(),]*maxrow for k in range(2)]\n",
        "\n",
        "if max(L0trainval[0])>samp:\n",
        "    print(\"Warning: a training recording is not included\")\n",
        "    print(np.where(L0trainval[0]>samp))\n",
        "# For train dataset, we fill the lines successyvely by adding the longer possible recording at the end of line until no recording fill\n",
        "i=0\n",
        "while min(L0trainval[0])<=samp:\n",
        "    X[0][i,0,5]=1\n",
        "    j=1\n",
        "    while samp-j-1>=min(L0trainval[0]):\n",
        "        k=np.argmax(L0trainval[0]*(L0trainval[0]<=samp-j-1))\n",
        "        ListRec[0][i]+=(k,)\n",
        "        LengthRec[0][i]+=(L0trainval[0][k],)\n",
        "        X[0][i,j:j+L0trainval[0][k],0:5]=X0trainval[0][k]\n",
        "        X[0][i,j+L0trainval[0][k],5]=1\n",
        "        Y[0][i,j:j+L0trainval[0][k],0:4]=Y0trainval[0][k]\n",
        "        j=j+L0trainval[0][k]+1\n",
        "        L0trainval[0][k]=samp+1\n",
        "\n",
        "    X[0][i,j:,5]=1\n",
        "    i=i+1\n",
        "X[0]=X[0][:i,:,:]\n",
        "Y[0]=Y[0][:i,:,:]\n",
        "ListRec[0]=ListRec[0][:i]\n",
        "LengthRec[0]=LengthRec[0][:i]\n",
        "\n",
        "# For validation dataset, we first put the longer recordings \n",
        "X[1]=X[1][:maxrowval,:,:]\n",
        "Y[1]=Y[1][:maxrowval,:,:]\n",
        "ListRec[1]=ListRec[1][:maxrowval]\n",
        "LengthRec[1]=LengthRec[1][:maxrowval]    \n",
        "CurrFill=np.ones(maxrowval,dtype=np.int32)\n",
        "X[1][:,0,5]=1\n",
        "while min(L0trainval[1])<=samp: #allocated rec hav L0 = samp+1\n",
        "    k=np.argmax(L0trainval[1]*(L0trainval[1]<=samp)) #choose longer rec not allocated\n",
        "    pos=np.argmin(CurrFill) # Search the less filled line\n",
        "    #print(\"pos %d rec %d of length %d\" % (pos,k,L0trainval[1][k]))\n",
        "    ListRec[1][pos]+=(k,)\n",
        "    LengthRec[1][pos]+=(L0trainval[1][k],)\n",
        "    X[1][pos,CurrFill[pos]:CurrFill[pos]+L0trainval[1][k],0:5]=X0trainval[1][k]\n",
        "    X[1][pos,CurrFill[pos]+L0trainval[1][k],5]=1\n",
        "    Y[1][pos,CurrFill[pos]:CurrFill[pos]+L0trainval[1][k],0:4]=Y0trainval[1][k]\n",
        "    CurrFill[pos]+=(L0trainval[1][k]+1)\n",
        "    L0trainval[1][k]=samp+1\n",
        "\n",
        "if np.any(L0trainval[1]!=samp+1):\n",
        "    print(\"Warning: a test recording is not included\")\n",
        "    print(np.where(L0trainval[1]!=samp+1))\n",
        "\n",
        "for i in range(maxrowval):\n",
        "    X[1][i,CurrFill[i]:,5]=1\n",
        "\n",
        "(X_train,Y_train,X_val,Y_val)=(X[0],Y[0],X[1],Y[1])\n",
        "Y_val[:,:,0]=Y_val[:,:,0]/np.sum(Y_val[:,:,0])\n",
        "Y_train[:,:,0]=Y_train[:,:,0]/np.sum(Y_train[:,:,0]) \n",
        "\n",
        "with open(basefolder+'dataV8dop%d-%d.pkl'%(X_train.shape[0],X_val.shape[0]), 'wb') as f:  \n",
        "    pickle.dump([X_train,Y_train,X_val,Y_val,ListRec,LengthRec], f)\n",
        "\n",
        "X_train.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80, 64800, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III For FSScalp"
      ],
      "metadata": {
        "id": "IjLC9z_3aWMZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPzmQZsOBeXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc62f1b-7770-4bec-edc5-2b58499de8e5"
      },
      "source": [
        "nd=227\n",
        "maxrow=200\n",
        "#Package the Fetal Scalp ECG signals in units of length \"samp\",\n",
        "# For TPU V3\n",
        "#samp=30600 # min for having 40 on train lines\n",
        "#maxrowval=40 \n",
        "\n",
        "#For TPU V2 batch 32\n",
        "#samp=38400\n",
        "#maxrowval=32 \n",
        "\n",
        "#For TPU V2 batch 24\n",
        "#samp=51000 \n",
        "#maxrowval=24 \n",
        "\n",
        "#For TPU V2 btach 16\n",
        "samp=38400 \n",
        "maxrowval=16 \n",
        "\n",
        "u8=array.array(\"B\")\n",
        "Lfile=os.path.getsize(\"dataV8/isvalint.dat\")\n",
        "with open(\"dataV8/isvalint.dat\", \"rb\") as binary_file:\n",
        "    u8.fromfile(binary_file,Lfile) \n",
        "isval0=np.asarray(u8.tolist())>0\n",
        "\n",
        "X0=();Y0=();L=();isval=();\n",
        "for i in range(nd):\n",
        "    Lt=os.path.getsize(\"dataV8/int\"+(\"%03d\" % (i+1))+\".int\")//6\n",
        "    u16=array.array(\"H\")\n",
        "    with open(\"dataV8/int\"+(\"%03d\" % (i+1))+\".int\", \"rb\") as binary_file:\n",
        "        u16.fromfile(binary_file,Lt*3)\n",
        "    data=np.asarray(u16.tolist()).reshape(-1,3)/4\n",
        "\n",
        "    isFalse=(data[:,1]<1)\n",
        "    docare=(data[:,1]!=1)           \n",
        "\n",
        "    if sum(docare)>0:\n",
        "        k=np.sqrt(len(docare)/(sum(docare)))\n",
        "        docare=docare*k       \n",
        "\n",
        "        Ynp=np.concatenate((docare.reshape(1,-1,1),isFalse.reshape(1,-1,1)),axis=2)\n",
        "        NotLoss=(data>0).astype(float)\n",
        "        data=np.concatenate(((data[:,0:1]-120)/60*NotLoss[:,0:1],NotLoss[:,0:1],data[:,2:3]),axis=1)\n",
        "        Y0+=(Ynp,)\n",
        "        X0+=(data,)   \n",
        "        isval+=(isval0[i],)\n",
        "        L+=(Lt,)\n",
        "\n",
        "X0trainval=[()  for k in range(2)];Y0trainval=[() for k in range(2)];L0trainval=[()  for k in range(2)];\n",
        "X0trainval[0]=[X0[k] for k in np.where(np.logical_not(isval))[0]]\n",
        "Y0trainval[0]=[Y0[k] for k in np.where(np.logical_not(isval))[0]]\n",
        "L0trainval[0]=np.asarray([L[k] for k in np.where(np.logical_not(isval))[0]])\n",
        "X0trainval[1]=[X0[k] for k in np.where(isval)[0]]\n",
        "Y0trainval[1]=[Y0[k] for k in np.where(isval)[0]]\n",
        "L0trainval[1]=np.asarray([L[k] for k in np.where(isval)[0]])\n",
        "\n",
        "nval=len(L0trainval[1])\n",
        "\n",
        "X=[np.zeros((maxrow,samp,4)) for k in range(2)]\n",
        "Y=[np.zeros((maxrow,samp,2)) for k in range(2)]\n",
        "\n",
        "ListRec=[ [(),]*maxrow for k in range(2)]\n",
        "LengthRec=[ [(),]*maxrow for k in range(2)]\n",
        "\n",
        "# For train dataset, we fill the lines successyvely by adding the longer possible recording at the end of line until no recording fill\n",
        "if max(L0trainval[0])>samp:\n",
        "    print(\"Warning: a training recording is not included\")\n",
        "    print(np.where(L0trainval[0]>samp))\n",
        "i=0\n",
        "while min(L0trainval[0])<=samp:\n",
        "    X[0][i,0,3]=1\n",
        "    j=1\n",
        "    while samp-j-1>=min(L0trainval[0]):\n",
        "        k=np.argmax(L0trainval[0]*(L0trainval[0]<=samp-j-1))\n",
        "        ListRec[0][i]+=(k,)\n",
        "        LengthRec[0][i]+=(L0trainval[0][k],)\n",
        "        X[0][i,j:j+L0trainval[0][k],0:3]=X0trainval[0][k]\n",
        "        X[0][i,j+L0trainval[0][k],3]=1\n",
        "        Y[0][i,j:j+L0trainval[0][k],0:2]=Y0trainval[0][k]\n",
        "        j=j+L0trainval[0][k]+1\n",
        "        L0trainval[0][k]=samp+1\n",
        "\n",
        "    X[0][i,j:,3]=1\n",
        "    i=i+1\n",
        "X[0]=X[0][:i,:,:]\n",
        "Y[0]=Y[0][:i,:,:]\n",
        "ListRec[0]=ListRec[0][:i]\n",
        "LengthRec[0]=LengthRec[0][:i]\n",
        "\n",
        "# For validation dataset, we first put the longer recordings \n",
        "X[1]=X[1][:maxrowval,:,:]\n",
        "Y[1]=Y[1][:maxrowval,:,:]\n",
        "ListRec[1]=ListRec[1][:maxrowval]\n",
        "LengthRec[1]=LengthRec[1][:maxrowval]    \n",
        "CurrFill=np.ones(maxrowval,dtype=np.int32)\n",
        "X[1][:,0,3]=1\n",
        "while min(L0trainval[1])<=samp: #allocated rec hav L0 = samp+1\n",
        "    k=np.argmax(L0trainval[1]*(L0trainval[1]<=samp)) #choose longer rec not allocated\n",
        "    pos=np.argmin(CurrFill) # Search the less filled line\n",
        "    #print(\"pos %d rec %d of length %d\" % (pos,k,L0trainval[1][k]))\n",
        "    ListRec[1][pos]+=(k,)\n",
        "    LengthRec[1][pos]+=(L0trainval[1][k],)\n",
        "    X[1][pos,CurrFill[pos]:CurrFill[pos]+L0trainval[1][k],0:3]=X0trainval[1][k]\n",
        "    X[1][pos,CurrFill[pos]+L0trainval[1][k],3]=1\n",
        "    Y[1][pos,CurrFill[pos]:CurrFill[pos]+L0trainval[1][k],0:2]=Y0trainval[1][k]\n",
        "    CurrFill[pos]+=(L0trainval[1][k]+1)\n",
        "    L0trainval[1][k]=samp+1\n",
        "if np.any(L0trainval[1]!=samp+1):\n",
        "    print(\"Warning: a test recording is not included\")\n",
        "    print(np.where(L0trainval[1]!=samp+1))\n",
        "\n",
        "for i in range(maxrowval):\n",
        "    X[1][i,CurrFill[i]:,3]=1\n",
        "\n",
        "(X_train,Y_train,X_val,Y_val)=(X[0],Y[0],X[1],Y[1])\n",
        "Y_val[:,:,0]=Y_val[:,:,0]/np.sum(Y_val[:,:,0])\n",
        "Y_train[:,:,0]=Y_train[:,:,0]/np.sum(Y_train[:,:,0]) \n",
        "\n",
        "with open(basefolder+'dataV8Int%d-%d.pkl'%(X_train.shape[0],X_val.shape[0]), 'wb') as f:  \n",
        "    pickle.dump([X_train,Y_train,X_val,Y_val,ListRec,LengthRec], f)\n",
        "    \n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 38400, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqXSK9P4p2Qb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}