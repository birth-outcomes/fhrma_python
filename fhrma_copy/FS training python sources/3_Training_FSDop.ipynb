{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3 Training FSDop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7F4Hin/+rHIw8faMBSoqz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsb-fmm/FHRMA/blob/master/FS%20training%20python%20sources/3_Training_FSDop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train FSDop model\n",
        "\n",
        "This public code is  not well tested\n",
        "contact samuel.boudet@univ-catholille.fr if you have trouble\n",
        "\n",
        "Associated paper in https://www.preprints.org/manuscript/202207.0131/v1\n",
        "\n",
        "FHR Morphological Analysis Toolbox Copyright (C) 2022 Samuel Boudet, Faculté de Médecine et Maïeutique, samuel.boudet@gmail.com\n",
        "\n",
        "This file is part of FHR Morphological Analysis Toolbox\n",
        "\n",
        "FHR Morphological Analysis Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
        "\n",
        "FHR Morphological Analysis Toolbox is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
        "\n",
        "You should have received a copy of the GNU General Public License along with this program. If not, see http://www.gnu.org/licenses/."
      ],
      "metadata": {
        "id": "BdCxyalJjvoy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06_C9MNpSKYn"
      },
      "source": [
        "import tensorflow as tf\n",
        "import array\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import pickle\n",
        "from IPython.core.debugger import set_trace\n",
        "from tensorflow.keras import layers as lay\n",
        "from tensorflow.keras import regularizers\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUuxUFPXSV-4"
      },
      "source": [
        "Mname='FSDop00'\n",
        "grufn=lay.GRU\n",
        "#for colab TPU (v2)\n",
        "#BatchPerEpoch=[24,8] \n",
        "#batch_size=24\n",
        "\n",
        "#for GPU\n",
        "BatchPerEpoch=[4,1] \n",
        "batch_size=20\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "basefolder=\"drive/My Drive/FHRMA-Training-FS/\"\n",
        "logfolder=\"logs/\"\n",
        "\n",
        "# For TPU v3 in google cloud\n",
        "#Mname='FSDop00'\n",
        "#samp=64800\n",
        "#batch_size=40\n",
        "#grufn=lay.GRU\n",
        "#BatchPerEpoch=[16,8]\n",
        "##8core*2steps*5batchsize\n",
        "\n",
        "#basefolder=\"\"\n",
        "#logfolder=\"gs://fhrfalsesig/logs/\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRbdre5kSkT4"
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except:\n",
        "    tpu = None\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    #tf.compat.v1.disable_eager_execution()\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
        "\n",
        "\n",
        "# If google cloud\n",
        "#tf.profiler.experimental.server.start(9012)\n",
        "#strategy = distribute_utils.get_distribution_strategy(\n",
        "#  distribution_strategy=\"tpu\",\n",
        "#  num_gpus=1,      \n",
        "#  tpu_address=\"fhrfalsesigtpu1\")\n",
        "  \n",
        "#strategy_scope = distribute_utils.get_strategy_scope(strategy)\n",
        "  \n",
        "#print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tQSb2lIUe3_"
      },
      "source": [
        "#with open(basefolder+'dataV8dop80-40.pkl','rb') as f: #TPU v3\n",
        "#with open(basefolder+'dataV8dop72-24.pkl','rb') as f: #TPU v2\n",
        "with open(basefolder+'dataV8dop80-20.pkl','rb') as f: #GPU\n",
        "    X_train, Y_train, X_val, Y_val, ListRec,LengthRec = pickle.load(f)\n",
        " \n",
        "batch_size=X_val.shape[0] #24 or 40\n",
        "samp=X_train.shape[1]\n",
        "\n",
        "Y_train[:,:,0:3:2]=Y_train[:,:,0:3:2]*BatchPerEpoch[0]\n",
        "Y_val[:,:,0:3:2]=Y_val[:,:,0:3:2]*BatchPerEpoch[1]\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPRgCsl1Uy6e"
      },
      "source": [
        "@tf.function \n",
        "def weighted_binary_crossentropy(yTrue, yPred): \n",
        "    Pred=yPred[:,:,0]\n",
        "    Pred=tf.clip_by_value(Pred,tf.constant(.00001),tf.constant(.999999))\n",
        "    isFalse=yTrue[:,:,1]\n",
        "    docare=yTrue[:,:,0]    \n",
        "    N=(tf.keras.backend.log(Pred)*isFalse+tf.keras.backend.log(tf.constant(1.)-Pred)*(tf.constant(1.)-isFalse))*docare\n",
        "    return -tf.keras.backend.sum(N)#/tf.keras.backend.sum(docare)\n",
        "\n",
        "@tf.function \n",
        "def weighted_accuracy(yTrue, yPred):\n",
        "    \n",
        "    Pred=yPred[:,:,0]\n",
        "    isFalse=yTrue[:,:,1]\n",
        "    docare=yTrue[:,:,0] \n",
        "    C=tf.cast((Pred>tf.constant(.5)),tf.float32)    \n",
        "    N=tf.keras.backend.sum( tf.cast((C==isFalse),tf.float32) * docare)\n",
        "    return tf.keras.backend.sum(N)#/tf.keras.backend.sum(docare)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHvoa7AhVSLd"
      },
      "source": [
        "class Split(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(Split, self).__init__()      \n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.split(inputs,num_or_size_splits=2, axis=0)\n",
        "\n",
        "class RevT(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(RevT, self).__init__()      \n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.keras.backend.reverse(inputs,1)\n",
        "\n",
        "class Set0forReset(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(Set0forReset, self).__init__()   \n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.concat([tf.multiply(inputs[:,:,0:-1],1.-inputs[:,:,-1,None]),inputs[:,:,-1,None]],axis=2)     \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NCzQcPuZRu_"
      },
      "source": [
        "L=(os.path.getsize(basefolder+\"DiffMF.dat\")//2)\n",
        "i16=array.array(\"h\")\n",
        "with open(basefolder+'DiffMF.dat', 'rb') as f:  \n",
        "    i16.fromfile(f,L)\n",
        "DiffFM=np.asarray(i16.tolist()).reshape(-1,1)/4/60\n",
        "DiffFM=np.concatenate((DiffFM,np.flip(DiffFM,0)),0)\n",
        "DiffFM=tf.constant(DiffFM[0:samp,:],dtype=tf.float32)\n",
        "def siglossgenerator(X,Y):\n",
        "    pMHRn=np.array([4,12,24,36,64,120])*2\n",
        "    pMHRp=np.array([.35,.35,.35,.35,.35,.35])\n",
        "    pMHRd=np.array([3000,1000,300,60,20,6])\n",
        "    pFHRn=np.array([4,12,24,36,64,120])*2\n",
        "    pFHRp=np.array([.25,.25,.25,.25,.25,.25])\n",
        "    pFHRd=np.array([2000,500,150,60,20,6])\n",
        "    \n",
        "    \n",
        "\n",
        "    pKeepMHR=.15\n",
        "    pKeepFHR=.20\n",
        "    pCut=.4\n",
        "    pNoStage2=.1\n",
        "\n",
        "    pMHR0=tf.constant(.1,dtype=tf.float32)\n",
        "    stdMult=tf.constant(.08,dtype=tf.float32)\n",
        "\n",
        "   \n",
        "    rg=tf.reshape(tf.range(0,samp,dtype=tf.int32),[samp,1])\n",
        "    if tf.random.uniform([])<pCut:\n",
        "        pos=tf.random.uniform([],maxval=samp-1,dtype=tf.int32)\n",
        "        newX=tf.tensor_scatter_nd_update(X, [[pos,0],[pos,1],[pos,2],[pos,3],[pos,4],[pos,5]], [0,0,0,0,0,1])\n",
        "        newY=tf.tensor_scatter_nd_update(Y, [[pos,0],[pos,1],[pos,2],[pos,3]], [0,0,0,0])\n",
        "    else:\n",
        "        newX=X\n",
        "        newY=Y  \n",
        "\n",
        "    FHR=newX[:,0:1]\n",
        "    maskFHR=newX[:,1:2]\n",
        "    MHR=newX[:,2:3]\n",
        "    maskMHR=newX[:,3:4]\n",
        "    isStage2=newX[:,4:5]\n",
        "    care=newY[:,0:1]\n",
        "    falsesig=newY[:,1:2]\n",
        "    careM=newY[:,2:3]\n",
        "    falsesigM=newY[:,3:4]\n",
        "    \n",
        "    if tf.random.uniform([])<pNoStage2:\n",
        "        isStage2=tf.zeros(tf.shape(isStage2),dtype=tf.dtypes.float32)\n",
        "            \n",
        "  \n",
        "    if tf.random.uniform([])>pKeepMHR:\n",
        "        for k in range(len(pMHRn)):\n",
        "            N=tf.cast(tf.floor(tf.math.abs(tf.random.normal([],0,pMHRn[k],dtype=tf.float32))),tf.int32)\n",
        "            for j in range(N):\n",
        "                if tf.random.uniform([])<pMHRp[k]: # removing maternal heart rate\n",
        "                    mid=tf.random.uniform([],maxval=samp,dtype=tf.int32)\n",
        "                    dur=tf.cast(abs(tf.random.normal([],mean=pMHRd[k],stddev=pMHRd[k]/2)),tf.int32)\n",
        "                    maskMHR=tf.where( (rg<mid-dur)|(rg>=mid+dur), maskMHR, [0])  \n",
        "                    careM=tf.where( (rg<mid-dur)|(rg>=mid+dur), careM, [0])    \n",
        "        \n",
        "        N=tf.cast(tf.floor(tf.math.abs(tf.random.normal([],0,50,dtype=tf.float32))),tf.int32)\n",
        "        for j in range(N):\n",
        "            mid=tf.random.uniform([],maxval=samp,dtype=tf.int32)\n",
        "            durT=tf.cast(abs(tf.random.normal([],mean=0,stddev=160/2)),tf.int32)\n",
        "            durPart=tf.cast(abs(tf.random.normal([],mean=0,stddev=80/2)),tf.int32)\n",
        "            OffsetPart=tf.cast(tf.random.normal([],mean=0,stddev=20),tf.int32)\n",
        "            endFalse=tf.clip_by_value(durPart+OffsetPart,-durT,durT);\n",
        "            startFalse=tf.clip_by_value(-durPart+OffsetPart,-durT,durT);\n",
        "\n",
        "            \n",
        "            r=tf.random.uniform([])\n",
        "            if r<.8:\n",
        "                change=tf.where( (rg>=mid+startFalse) & (rg<mid+endFalse) & (maskMHR==1.) & (MHR<0.1), [1.], [0.] )\n",
        "                MHR=(MHR+2)*(1+change)-2\n",
        "            else:\n",
        "                change=tf.where( (rg>=mid+startFalse) & (rg<mid+endFalse) & (maskMHR==1.) & (MHR>-1.3), [1.], [0.] )\n",
        "                MHR=(MHR+2)*(1-.5*change)-2\n",
        "\n",
        "            falsesigM=tf.where( (change==1) & (falsesigM==0) & (careM>0) ,[1.],falsesigM )\n",
        "            maskMHR=tf.where( (rg<mid-durT)|(rg>=mid+durT)|(change==1.), maskMHR, [0])\n",
        "            careM=tf.where( (rg<mid-durT)|(rg>=mid+durT)|(change==1.), careM*(1+change), [0]) \n",
        "\n",
        "    if tf.random.uniform([])>pKeepFHR:\n",
        "        for k in range(len(pFHRn)):\n",
        "            N=tf.cast(tf.floor(tf.math.abs(tf.random.normal([],0,pFHRn[k],dtype=tf.float32))),tf.int32)\n",
        "            for j in range(N):\n",
        "                if tf.random.uniform([])<pFHRp[k]: # removing maternal heart rate\n",
        "                    mid=tf.random.uniform([],maxval=samp,dtype=tf.int32)\n",
        "                    dur=tf.cast(abs(tf.random.normal([],mean=pFHRd[k],stddev=pFHRd[k]/2)),tf.int32)\n",
        "                    maskFHR=tf.where( (rg<mid-dur)|(rg>=mid+dur), maskFHR, [0]) \n",
        "                    care=tf.where( (rg<mid-dur)|(rg>=mid+dur), care, [0]) \n",
        "\n",
        "        N=tf.cast(tf.floor(tf.math.abs(tf.random.normal([],0,100,dtype=tf.float32))),tf.int32)\n",
        "        for j in range(N):\n",
        "            mid=tf.random.uniform([],maxval=samp,dtype=tf.int32)\n",
        "            durT=tf.cast(abs(tf.random.normal([],mean=120,stddev=120/2)),tf.int32)\n",
        "            durPart=tf.cast(abs(tf.random.normal([],mean=60,stddev=60/2)),tf.int32)\n",
        "            OffsetPart=tf.cast(tf.random.normal([],mean=0,stddev=20),tf.int32)\n",
        "            endFalse=tf.clip_by_value(durPart+OffsetPart,-durT,durT);\n",
        "            startFalse=tf.clip_by_value(-durPart+OffsetPart,-durT,durT);\n",
        "            r=tf.random.uniform([])\n",
        "            if r<.5:\n",
        "                change=tf.where( (rg>=mid+startFalse) & (rg<mid+endFalse) & (falsesig==0.)& (care>0.), [1.], [0.] )\n",
        "            else:\n",
        "                change=tf.where( (rg>=mid+startFalse) & (rg<mid+endFalse) & (falsesig==0.)& (care>0.) & (maskMHR==1.), [1.], [0.] )\n",
        "\n",
        "            falsesig=falsesig+change\n",
        "            maskFHR=tf.where( (rg<mid-durT)|(rg>=mid+durT)|(change==1.), maskFHR, [0])\n",
        "            care=tf.where( (rg<mid-durT)|(rg>=mid+durT)|(change==1.), care*(1+change), [0]) \n",
        "            if r<.3:\n",
        "                FHR=(FHR+2)*(1+change)-2\n",
        "            elif r<.5:\n",
        "                FHR=(FHR+2)*(1-.5*change)-2\n",
        "            elif r<.9:\n",
        "                FHR=(FHR+2)*(1-change)+(MHR+2+DiffFM)*change-2\n",
        "            elif r<.95:\n",
        "                FHR=(FHR+2)*(1-change)+(MHR+2+DiffFM)*2*change-2\n",
        "            else:\n",
        "                FHR=(FHR+2)*(1-change)+(MHR+2+DiffFM)*.5*change-2\n",
        "\n",
        "            if r>.5 and tf.random.uniform([])<.5:\n",
        "                durMHRLost=tf.cast(abs(tf.random.normal([],mean=0,stddev=240)),tf.int32)\n",
        "                OffsetMHRLost=tf.cast(tf.random.normal([],mean=0,stddev=120),tf.int32)\n",
        "                maskMHR=tf.where( (rg<mid+OffsetMHRLost-durMHRLost)|(rg>=mid+OffsetMHRLost+durMHRLost), maskMHR, [0])   \n",
        "                careM=tf.where( (rg<mid+OffsetMHRLost-durMHRLost)|(rg>=mid+OffsetMHRLost+durMHRLost), careM, [0])   \n",
        "\n",
        "    maskFHR=tf.where( FHR<=2.25, maskFHR, [0])   \n",
        "    if tf.random.uniform([])<pMHR0: \n",
        "        maskMHR=tf.zeros([samp,1],dtype=tf.float32)\n",
        "        careM=tf.zeros([samp,1],dtype=tf.float32)    \n",
        "    \n",
        "    S=1.+tf.random.normal([],stddev=stdMult)\n",
        "\n",
        "    newX=tf.concat([ ((FHR+2)*S-2)*maskFHR , maskFHR, ((MHR+2)*S-2)*maskMHR,maskMHR,isStage2,newX[:,5:6] ],axis=1)\n",
        "    \n",
        "    newX=tf.ensure_shape(newX,X.shape)\n",
        "    newY=tf.concat([ care,falsesig,careM,falsesigM ],axis=1)\n",
        "    return newX,newY\n",
        "\n",
        "\n",
        "traindataset=tf.data.Dataset.from_tensor_slices(    (  tf.constant(X_train,dtype=tf.float32)  ,  tf.constant(Y_train,dtype=tf.float32)  )    )\n",
        "traindataset=traindataset.shuffle(X_train.shape[0],reshuffle_each_iteration=True).repeat()\n",
        "traindataset=traindataset.map(siglossgenerator)\n",
        "traindataset=traindataset.batch(batch_size,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "valdataset=tf.data.Dataset.from_tensor_slices((X_val,Y_val)).cache().repeat().batch(batch_size,drop_remainder=True)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZdQ1Bw6gM71"
      },
      "source": [
        "class Sparsity(tf.keras.constraints.Constraint):\n",
        "    def __init__(self, n,h):\n",
        "        self. n = n    \n",
        "        self.h=h\n",
        "        m=np.zeros([h,h])\n",
        "        for i in range(int(h/n)):\n",
        "            m[i*n:i*n+n,i*n:i*n+n]=1.\n",
        "        self.mask=tf.constant(np.concatenate([m,m,m],axis=1),dtype=tf.float32)\n",
        "        \n",
        "    def __call__(self, w):\n",
        "        w.assign(w*self.mask)\n",
        "        return w\n",
        "\n",
        "class SparsityKernel(tf.keras.constraints.Constraint):\n",
        "    def __init__(self, m,n,slices):\n",
        "        #m input size ; n state size\n",
        "        #slices\n",
        "        ma1=np.zeros([m,n],dtype=np.float32)\n",
        "        for i in range(slices.shape[0]):\n",
        "            ma1[slices[i,0]:slices[i,1],slices[i,2]:slices[i,3]]=1.\n",
        "        self.mask=tf.constant(np.concatenate([ma1,ma1,ma1],axis=1),dtype=tf.float32)\n",
        "        ma2=np.zeros([m,3*n])\n",
        "        ma2[m-1,n:2*n]=-1.e30\n",
        "        ma2[m-1,0:n]=-1.e30\n",
        "        self.maskReset=tf.constant(ma2,dtype=tf.float32)\n",
        "    def __call__(self, w):\n",
        "        w.assign(w*self.mask+self.maskReset)\n",
        "        return w\n",
        "\n",
        "class Reseter(tf.keras.constraints.Constraint):\n",
        "    def __init__(self, m,n):\n",
        "        ma=np.zeros([m,3*n])\n",
        "        ma[m-1,n:2*n]=-1.e30\n",
        "        ma[m-1,0:n]=-1.e30\n",
        "        self.mask=tf.constant(ma,dtype=tf.float32)\n",
        "        \n",
        "    def __call__(self, w):\n",
        "        w.assign(w+self.mask)\n",
        "        return w\n",
        "\n",
        "class BiasConstraintCallback(tf.keras.callbacks.Callback):\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        for GRUlayer in [\"GRU1\",\"GRU2\",\"GRU3\"]:\n",
        "            W=self.model.get_layer(GRUlayer).get_weights()\n",
        "            L=int(len(W[0][-1])/3)\n",
        "            W[0][-1][2*L:3*L]=-W[2][0][2*L:3*L]#-W[2][1][2*L:3*L] #Pour CudNN a priori W[2][2*L:3*L]-W[2][5*L:6*L] Mais à vérifier ordre des 3 couches\n",
        "            self.model.get_layer(GRUlayer).set_weights(W)\n",
        "            "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe6MIZA5gYig"
      },
      "source": [
        "\n",
        "def create_model(batch_size,siglen):\n",
        "    I=lay.Input(batch_shape= (batch_size, siglen, 6), name=\"SigInput\") \n",
        "\n",
        "    RevI=RevT()(I)\n",
        "    IforwBack=lay.concatenate([I,RevI], axis=0)\n",
        "    slicesKernel0=np.array([[2,6,0,12]])\n",
        "\n",
        "    GRU1M=grufn(12,return_sequences=True,recurrent_initializer='glorot_uniform'\n",
        "        ,stateful=False,recurrent_constraint=Sparsity(4,12),kernel_constraint=SparsityKernel(6,12,slicesKernel0),name='GRU1MHR')(IforwBack)\n",
        "    GRU1M=lay.GaussianDropout(.2)(GRU1M)\n",
        "\n",
        "    Lay0Forw,Lay0Backw=Split()(GRU1M)\n",
        "    PMat=lay.Dense(1,activation='sigmoid',name=\"DensePmat\")(lay.concatenate([Lay0Forw,RevT()(Lay0Backw)], axis=2))\n",
        "    RPMat=RevT()(PMat)\n",
        "    Lay0=Set0forReset()(     lay.concatenate(   [lay.concatenate([PMat,I], axis=2), lay.concatenate([RPMat,RevI], axis=2)],axis=0  )     )\n",
        "    \n",
        "\n",
        "    slicesKernel1=np.array([[1,3,0,12],[0,1,12,24],[3,5,12,24],[5,7,0,24]])\n",
        "    GRU1Dop=grufn(24,return_sequences=True,recurrent_initializer='glorot_uniform'\n",
        "        ,stateful=False,recurrent_constraint=Sparsity(4,24),kernel_constraint=SparsityKernel(7,24,slicesKernel1),name='GRU1')(Lay0)\n",
        "    GRU1Dop=lay.GaussianDropout(.2)(GRU1Dop)\n",
        "    Lay1Forw,Lay1Backw=Split()(GRU1Dop)\n",
        "    Lay1=Set0forReset()(     lay.concatenate(  [lay.concatenate([Lay1Forw,RevT()(Lay1Backw),PMat,I], axis=2), lay.concatenate([RevT()(Lay1Forw),Lay1Backw,RPMat,RevI], axis=2)],axis=0  )     )\n",
        "    \n",
        "\n",
        "    slicesKernel2=np.array([[0,4,0,8],[12,16,0,8],[24,28,0,8],[36,40,0,8]\n",
        "        ,[4,8,8,16],[16,20,8,16],[28,32,8,16],[40,44,8,16]\n",
        "        ,[8,12,16,24],[20,24,16,24],[32,36,16,24],[44,48,16,24]\n",
        "        ,[48,55,0,24]])\n",
        "    GRU2Dop=grufn(24,return_sequences=True,recurrent_initializer='glorot_uniform'\n",
        "        ,stateful=False,recurrent_constraint=Sparsity(4,24),kernel_constraint=SparsityKernel(55,24,slicesKernel2),name='GRU2')(Lay1)\n",
        "    GRU2Dop=lay.GaussianDropout(.3)(GRU2Dop)\n",
        "    Lay2Forw,Lay2Backw=Split()(GRU2Dop)\n",
        "    Lay2=Set0forReset()(     lay.concatenate(  [lay.concatenate([Lay2Forw,RevT()(Lay2Backw),PMat,I], axis=2), lay.concatenate([RevT()(Lay2Forw),Lay2Backw,RPMat,RevI], axis=2)],axis=0  )     )\n",
        "    \n",
        "    GRU3Dop=grufn(24,return_sequences=True,recurrent_initializer='glorot_uniform'\n",
        "        ,stateful=False,recurrent_constraint=Sparsity(8,24),kernel_constraint=Reseter(55,24),name='GRU3')(Lay2)\n",
        "    GRU3Dop=lay.GaussianDropout(.4)(GRU3Dop)\n",
        "    Lay3Forw,Lay3Backw=Split()(GRU3Dop)\n",
        "    CDop=lay.concatenate([Lay3Forw,RevT()(Lay3Backw)], axis=2)\n",
        "    \n",
        "    PDop=lay.Dense(1,activation='sigmoid',name=\"Dense1\")(CDop)\n",
        "    Out=lay.concatenate([PDop,PMat],axis=2)\n",
        "    model = tf.keras.Model(inputs=[I], outputs=[Out])\n",
        "\n",
        "\n",
        "    #model.summary()\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhjCGRf7jZ25"
      },
      "source": [
        "callbacks=[tf.keras.callbacks.TensorBoard(log_dir = logfolder+Mname , histogram_freq = 0)]\n",
        "#callbacks=[]\n",
        "callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath=basefolder+Mname+\"/best-{epoch:05d}-{val_loss:.4f}.h5\",\n",
        "                                                 save_best_only=True,\n",
        "                                                 verbose=1))\n",
        "callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath=basefolder+Mname+\"/save-{epoch:05d}.h5\",\n",
        "                                                 save_freq=200,\n",
        "                                                 verbose=1))\n",
        "def scheduler(epoch):\n",
        "    if epoch<100:\n",
        "        return 1.e-2\n",
        "    elif epoch<300:\n",
        "        return 5.e-3\n",
        "    elif epoch<1000:\n",
        "        return 2.e-3\n",
        "    else:\n",
        "        return 1.e-3\n",
        "callbacks.append(tf.keras.callbacks.LearningRateScheduler(scheduler))\n",
        "callbacks.append(BiasConstraintCallback())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load previously trainer FSMHR to set the corresponding weights in the new FSDop model\n",
        "def create_model_MHR(batch_size,siglen):\n",
        "    I=lay.Input(batch_shape= (batch_size, siglen, 6), name=\"SigInput\") \n",
        "\n",
        "    RevI=RevT()(I)\n",
        "    IforwBack=lay.concatenate([I,RevI], axis=0)\n",
        "    slicesKernel0=np.array([[2,6,0,12]])\n",
        "\n",
        "    GRU1M=grufn(12,return_sequences=True,recurrent_initializer='glorot_uniform'\n",
        "        ,stateful=False,recurrent_constraint=Sparsity(4,12),kernel_constraint=SparsityKernel(6,12,slicesKernel0),name='GRU1MHR')(IforwBack)\n",
        "    GRU1M=lay.GaussianDropout(.2)(GRU1M)\n",
        "\n",
        "    Lay0Forw,Lay0Backw=Split()(GRU1M)\n",
        "    PMat=lay.Dense(1,activation='sigmoid',name=\"DensePmat\")(lay.concatenate([Lay0Forw,RevT()(Lay0Backw)], axis=2))\n",
        "\n",
        "    model = tf.keras.Model(inputs=[I], outputs=[PMat])\n",
        "    return model\n",
        "modelMHR=create_model_MHR(1,5)\n",
        "!wget https://github.com/utsb-fmm/FHRMA/raw/master/FS%20training%20python%20sources/FSMHR.h5\n",
        "modelMHR.load_weights(\"FSMHR.h5\")\n",
        "GRU1MHR=modelMHR.get_layer(\"GRU1MHR\").get_weights()\n",
        "DensePmat=modelMHR.get_layer(\"DensePmat\").get_weights()"
      ],
      "metadata": {
        "id": "Ym3RVniGgsgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj3Dh2eySktA"
      },
      "source": [
        "initial_epoch=50000\n",
        "with strategy.scope():\n",
        "    model=create_model(int(batch_size/strategy.num_replicas_in_sync),samp)\n",
        "    model.get_layer(\"GRU1MHR\").set_weights(GRU1MHR)\n",
        "    model.get_layer(\"DensePmat\").set_weights(DensePmat)\n",
        "    #model.load_weights(basefolder+Mname+\"/save-00000.h5\")    \n",
        "    model.get_layer(\"GRU1MHR\").trainable=False\n",
        "    model.get_layer(\"DensePmat\").trainable=False\n",
        "    model.compile(\n",
        "        optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss=weighted_binary_crossentropy,\n",
        "        metrics=[weighted_accuracy])\n",
        "    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One epoch to create logs folders for tensorboard\n",
        "model.fit(traindataset,\n",
        "    epochs=1,\n",
        "    steps_per_epoch=np.floor(X_train.shape[0]/batch_size),\n",
        "    validation_freq=1,\n",
        "    initial_epoch=initial_epoch,\n",
        "    validation_steps=1,\n",
        "    validation_data=valdataset,\n",
        "    callbacks=callbacks    \n",
        ")\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/"
      ],
      "metadata": {
        "id": "m8YEa7L7iZxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STsMNbuVlRyo"
      },
      "source": [
        "model.fit(traindataset,\n",
        "    epochs=60000,\n",
        "    steps_per_epoch=np.floor(X_train.shape[0]/batch_size),\n",
        "    validation_freq=1,\n",
        "    initial_epoch=initial_epoch+1,\n",
        "    validation_steps=1,\n",
        "    validation_data=valdataset,\n",
        "    callbacks=callbacks    \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If you want to load and test the trained model\n",
        "\n",
        "!wget https://github.com/utsb-fmm/FHRMA/raw/master/FS%20training%20python%20sources/FSDop.h5\n",
        "model.load_weights(\"FSDop.h5\")\n",
        "model.evaluate(X_val,Y_val)"
      ],
      "metadata": {
        "id": "BFw4ZJU4jNgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export to Matlab? first removing reset channel\n",
        "GRU1MHR=model.get_layer('GRU1MHR').get_weights()\n",
        "GRU1MHR[0]=GRU1MHR[0][0:-1,:]\n",
        "DensePmat=model.get_layer('DensePmat').get_weights()\n",
        "GRU1=model.get_layer('GRU1').get_weights()\n",
        "GRU1[0]=GRU1[0][0:-1,:]\n",
        "GRU2=model.get_layer('GRU2').get_weights()\n",
        "GRU2[0]=GRU2[0][0:-1,:]\n",
        "GRU3=model.get_layer('GRU3').get_weights()\n",
        "GRU3[0]=GRU3[0][0:-1,:]\n",
        "Dense1=model.get_layer('Dense1').get_weights()\n",
        "from scipy.io import savemat\n",
        "mdic = {\"GRU1MHR\": GRU1MHR, \"DensePmat\": DensePmat,\"GRU1\":GRU1,\"GRU2\":GRU2,\"GRU3\":GRU3,\"Dense1\":Dense1}\n",
        "savemat(basefolder+\"FSDop.mat\", mdic)"
      ],
      "metadata": {
        "id": "wp021DfEmc1D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}